{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/train.tsv', sep='\\t', header=0)\n",
    "dataset['category_name'] = dataset['category_name'].fillna('Other').astype(str)\n",
    "dataset['brand_name'] = dataset['brand_name'].fillna('missing').astype(str)\n",
    "dataset['shipping'] = dataset['shipping'].astype(str)  # makes this categorical\n",
    "dataset['item_condition_id'] = dataset['item_condition_id'].astype(str)\n",
    "dataset['item_description'] = dataset['item_description'].fillna('None')\n",
    "\n",
    "X = dataset.loc[:, dataset.columns != 'price']\n",
    "Y = np.log1p(dataset['price'])\n",
    "\n",
    "# 80% training data, 20% test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "         train_id                                      name item_condition_id  \\\n604635     604635                                      Enzo                 3   \n496799     496799                               Black dress                 3   \n1035231   1035231                      2 items for Brittany                 1   \n628659     628659                               Texas budle                 2   \n261459     261459                      North face rain coat                 3   \n959361     959361                Michael kors crossbody bag                 3   \n199415     199415                      Cream studded wedges                 3   \n1460850   1460850                 Pink and gold trinket box                 3   \n568899     568899  True religion button shirt slimsize XXXL                 3   \n1365752   1365752        WD Scorpio Blue 1TB 2.5 hard drive                 3   \n\n                                             category_name  \\\n604635                           Women/Shoes/Mules & Clogs   \n496799                           Women/Dresses/Full-Length   \n1035231  Electronics/Cell Phones & Accessories/Cables &...   \n628659                       Women/Tops & Blouses/T-Shirts   \n261459                      Women/Athletic Apparel/Jackets   \n959361        Women/Women's Handbags/Messenger & Crossbody   \n199415                                 Women/Shoes/Sandals   \n1460850                 Home/Home Décor/Home Décor Accents   \n568899                                   Men/Tops/T-shirts   \n1365752  Electronics/Computers & Tablets/Drives, Storag...   \n\n                        brand_name shipping  \\\n604635                     missing        0   \n496799                     missing        1   \n1035231                    missing        0   \n628659                     missing        0   \n261459              The North Face        0   \n959361                Michael Kors        0   \n199415                     missing        0   \n1460850                    missing        0   \n568899   True Religion Brand Jeans        1   \n1365752            Western Digital        1   \n\n                                          item_description  \n604635   Enzo Angiolini Mules/Clogs. Super Cute with so...  \n496799   Xl long black dress Solid under with lace over...  \n1035231  - Urban Decay Eyeshadow This has never been us...  \n628659                                Very cute!! No flaws  \n261459   Sea foam green rain coat Size 18 XL in youth I...  \n959361                       Used , the color is rose gold  \n199415   The brand is Hot Rated purchased at Buckle. I ...  \n1460850  5.75\" tall and 4.5\" wide. Heavy resin. The fel...  \n568899   Please remember the brand name usually run sma...  \n1365752  Used like new condition Can be used on PS4 (yo...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train_id</th>\n      <th>name</th>\n      <th>item_condition_id</th>\n      <th>category_name</th>\n      <th>brand_name</th>\n      <th>shipping</th>\n      <th>item_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>604635</th>\n      <td>604635</td>\n      <td>Enzo</td>\n      <td>3</td>\n      <td>Women/Shoes/Mules &amp; Clogs</td>\n      <td>missing</td>\n      <td>0</td>\n      <td>Enzo Angiolini Mules/Clogs. Super Cute with so...</td>\n    </tr>\n    <tr>\n      <th>496799</th>\n      <td>496799</td>\n      <td>Black dress</td>\n      <td>3</td>\n      <td>Women/Dresses/Full-Length</td>\n      <td>missing</td>\n      <td>1</td>\n      <td>Xl long black dress Solid under with lace over...</td>\n    </tr>\n    <tr>\n      <th>1035231</th>\n      <td>1035231</td>\n      <td>2 items for Brittany</td>\n      <td>1</td>\n      <td>Electronics/Cell Phones &amp; Accessories/Cables &amp;...</td>\n      <td>missing</td>\n      <td>0</td>\n      <td>- Urban Decay Eyeshadow This has never been us...</td>\n    </tr>\n    <tr>\n      <th>628659</th>\n      <td>628659</td>\n      <td>Texas budle</td>\n      <td>2</td>\n      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n      <td>missing</td>\n      <td>0</td>\n      <td>Very cute!! No flaws</td>\n    </tr>\n    <tr>\n      <th>261459</th>\n      <td>261459</td>\n      <td>North face rain coat</td>\n      <td>3</td>\n      <td>Women/Athletic Apparel/Jackets</td>\n      <td>The North Face</td>\n      <td>0</td>\n      <td>Sea foam green rain coat Size 18 XL in youth I...</td>\n    </tr>\n    <tr>\n      <th>959361</th>\n      <td>959361</td>\n      <td>Michael kors crossbody bag</td>\n      <td>3</td>\n      <td>Women/Women's Handbags/Messenger &amp; Crossbody</td>\n      <td>Michael Kors</td>\n      <td>0</td>\n      <td>Used , the color is rose gold</td>\n    </tr>\n    <tr>\n      <th>199415</th>\n      <td>199415</td>\n      <td>Cream studded wedges</td>\n      <td>3</td>\n      <td>Women/Shoes/Sandals</td>\n      <td>missing</td>\n      <td>0</td>\n      <td>The brand is Hot Rated purchased at Buckle. I ...</td>\n    </tr>\n    <tr>\n      <th>1460850</th>\n      <td>1460850</td>\n      <td>Pink and gold trinket box</td>\n      <td>3</td>\n      <td>Home/Home Décor/Home Décor Accents</td>\n      <td>missing</td>\n      <td>0</td>\n      <td>5.75\" tall and 4.5\" wide. Heavy resin. The fel...</td>\n    </tr>\n    <tr>\n      <th>568899</th>\n      <td>568899</td>\n      <td>True religion button shirt slimsize XXXL</td>\n      <td>3</td>\n      <td>Men/Tops/T-shirts</td>\n      <td>True Religion Brand Jeans</td>\n      <td>1</td>\n      <td>Please remember the brand name usually run sma...</td>\n    </tr>\n    <tr>\n      <th>1365752</th>\n      <td>1365752</td>\n      <td>WD Scorpio Blue 1TB 2.5 hard drive</td>\n      <td>3</td>\n      <td>Electronics/Computers &amp; Tablets/Drives, Storag...</td>\n      <td>Western Digital</td>\n      <td>1</td>\n      <td>Used like new condition Can be used on PS4 (yo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 17.9 s, total: 5min 17s\n",
      "Wall time: 5min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": "<1186028x155839 sparse matrix of type '<class 'numpy.float64'>'\n\twith 59679331 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "preprocessor = CountVectorizer().build_preprocessor()\n",
    "\n",
    "\n",
    "def build_field_preprocessor(field):\n",
    "    field_idx = list(X_train.columns).index(field)\n",
    "    return lambda x: preprocessor(x[field_idx])  # this preprocesses like stripping accents, etc.\n",
    "\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        ngram_range=(1, 2),  # extract words and pairs of words\n",
    "        max_features=50_000,\n",
    "        preprocessor=build_field_preprocessor('name')\n",
    "    )),\n",
    "    ('category_name', CountVectorizer(\n",
    "        token_pattern='.+',  # separate by space\n",
    "        preprocessor=build_field_preprocessor('category_name')\n",
    "    )),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',  # separate by space\n",
    "        preprocessor=build_field_preprocessor('brand_name')\n",
    "    )),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',  # decimal numbers\n",
    "        preprocessor=build_field_preprocessor('shipping')\n",
    "    )),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',  # decimal numbers\n",
    "        preprocessor=build_field_preprocessor('item_condition_id')\n",
    "    )),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),  # up to three words in a sequence\n",
    "        max_features=1_000_00,\n",
    "        preprocessor=build_field_preprocessor('item_description')\n",
    "    ))\n",
    "])\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train.values)\n",
    "X_test_transformed = vectorizer.transform(X_test.values)  # don't fit to the test\n",
    "X_train_transformed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 34957628231419.32, NNZs: 155839, Bias: -378398429.463090, T: 1186028, Avg. loss: 1279671156143664027978956800.000000\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25963532078586.90, NNZs: 155839, Bias: -93381717.351799, T: 2372056, Avg. loss: 677575702517766765085196288.000000\n",
      "Total training time: 1.74 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23921512310695.80, NNZs: 155839, Bias: 226543099.779182, T: 3558084, Avg. loss: 427746372513961463991238656.000000\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 21445184982683.36, NNZs: 155839, Bias: -70529423.093777, T: 4744112, Avg. loss: 368685835142318074483441664.000000\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 20792540457743.72, NNZs: 155839, Bias: -111933035.863538, T: 5930140, Avg. loss: 308481888017009987633020928.000000\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 19224708703775.84, NNZs: 155839, Bias: -230453081.116020, T: 7116168, Avg. loss: 289943821827839374294777856.000000\n",
      "Total training time: 4.92 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 19066256182937.32, NNZs: 155839, Bias: -128998588.489837, T: 8302196, Avg. loss: 253547479589810983816658944.000000\n",
      "Total training time: 5.69 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 17845731205507.27, NNZs: 155839, Bias: 32869068.545190, T: 9488224, Avg. loss: 245547531617820900929306624.000000\n",
      "Total training time: 6.45 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 17843390151803.59, NNZs: 155839, Bias: 34213407.530368, T: 10674252, Avg. loss: 221200578752531519904940032.000000\n",
      "Total training time: 7.26 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 16889206607943.66, NNZs: 155839, Bias: -32860078.929564, T: 11860280, Avg. loss: 217504382256092374785589248.000000\n",
      "Total training time: 8.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 16986396143371.78, NNZs: 155839, Bias: -46745186.256474, T: 13046308, Avg. loss: 198316988484920622469611520.000000\n",
      "Total training time: 8.84 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 16100664541848.17, NNZs: 155839, Bias: -193065550.534372, T: 14232336, Avg. loss: 196459395252270020708794368.000000\n",
      "Total training time: 9.64 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 16289349012648.96, NNZs: 155839, Bias: 99503542.879856, T: 15418364, Avg. loss: 180411824630270798069760000.000000\n",
      "Total training time: 10.43 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 15472643841706.05, NNZs: 155839, Bias: -163652402.870961, T: 16604392, Avg. loss: 181293537604415368313438208.000000\n",
      "Total training time: 11.21 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 15592888515065.53, NNZs: 155839, Bias: 130961165.812882, T: 17790420, Avg. loss: 167478779299159143455129600.000000\n",
      "Total training time: 11.99 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 14907348699636.86, NNZs: 155839, Bias: -102647386.544929, T: 18976448, Avg. loss: 167943936620690329342836736.000000\n",
      "Total training time: 12.76 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 15150776608303.08, NNZs: 155839, Bias: 97020725.133666, T: 20162476, Avg. loss: 156544217772453647425208320.000000\n",
      "Total training time: 13.56 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 14482590914401.54, NNZs: 155839, Bias: -9353366.578487, T: 21348504, Avg. loss: 158904611039370023765278720.000000\n",
      "Total training time: 14.37 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 14670384918596.89, NNZs: 155839, Bias: -147254651.849260, T: 22534532, Avg. loss: 147927784963888306320310272.000000\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 14097018118584.72, NNZs: 155839, Bias: -18725113.045392, T: 23720560, Avg. loss: 149567151398769818362445824.000000\n",
      "Total training time: 16.09 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 14302246688267.22, NNZs: 155839, Bias: -152234051.380717, T: 24906588, Avg. loss: 140192959919825679139471360.000000\n",
      "Total training time: 16.96 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 13777251851358.20, NNZs: 155839, Bias: -105855741.683603, T: 26092616, Avg. loss: 142954887678620312203689984.000000\n",
      "Total training time: 17.79 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 13925503359759.32, NNZs: 155839, Bias: -83434074.736170, T: 27278644, Avg. loss: 134023895620072900397105152.000000\n",
      "Total training time: 18.67 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 13440303402056.31, NNZs: 155839, Bias: 66814705.953703, T: 28464672, Avg. loss: 135720599335141303019110400.000000\n",
      "Total training time: 19.60 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 13654298340769.72, NNZs: 155839, Bias: 29806558.985816, T: 29650700, Avg. loss: 128083722554565911946199040.000000\n",
      "Total training time: 20.41 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 13183121729273.13, NNZs: 155839, Bias: -44374270.682414, T: 30836728, Avg. loss: 129568272320019001365233664.000000\n",
      "Total training time: 21.18 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 13378314125774.56, NNZs: 155839, Bias: -196200831.492768, T: 32022756, Avg. loss: 123254158074561506058436608.000000\n",
      "Total training time: 21.94 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 12956110394016.53, NNZs: 155839, Bias: -88951984.090473, T: 33208784, Avg. loss: 125401209662517650537840640.000000\n",
      "Total training time: 22.71 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 13106277733616.71, NNZs: 155839, Bias: -73632483.335679, T: 34394812, Avg. loss: 118743752556429862116524032.000000\n",
      "Total training time: 24.11 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 12682060876287.81, NNZs: 155839, Bias: 25875606.474058, T: 35580840, Avg. loss: 120693186991833607615545344.000000\n",
      "Total training time: 25.06 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 12909699382760.17, NNZs: 155839, Bias: 20654480.712292, T: 36766868, Avg. loss: 114009219131269771027283968.000000\n",
      "Total training time: 26.08 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 12526008674684.43, NNZs: 155839, Bias: -48952024.929327, T: 37952896, Avg. loss: 116300297591713697118552064.000000\n",
      "Total training time: 26.94 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 12665760149753.31, NNZs: 155839, Bias: -172252469.081606, T: 39138924, Avg. loss: 110769909085586490658914304.000000\n",
      "Total training time: 27.72 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 12294969726408.35, NNZs: 155839, Bias: -88315450.736438, T: 40324952, Avg. loss: 112532908448396600928108544.000000\n",
      "Total training time: 28.53 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 12455208914731.83, NNZs: 155839, Bias: -85717064.057443, T: 41510980, Avg. loss: 107459457999784504633327616.000000\n",
      "Total training time: 29.48 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 12109800784686.14, NNZs: 155839, Bias: 105586634.322816, T: 42697008, Avg. loss: 109370488466794258232246272.000000\n",
      "Total training time: 30.56 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 12318982577614.71, NNZs: 155839, Bias: -68791622.144278, T: 43883036, Avg. loss: 103969846456678484025540608.000000\n",
      "Total training time: 31.35 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 11943256489284.59, NNZs: 155839, Bias: -156087999.215743, T: 45069064, Avg. loss: 106167568989454979355901952.000000\n",
      "Total training time: 32.11 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 12119797762790.95, NNZs: 155839, Bias: -148805140.493975, T: 46255092, Avg. loss: 101071319885929420265881600.000000\n",
      "Total training time: 32.90 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 11803644199637.20, NNZs: 155839, Bias: 9959115.358904, T: 47441120, Avg. loss: 102876038127865957600722944.000000\n",
      "Total training time: 33.71 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 11949829477725.62, NNZs: 155839, Bias: -84775039.558351, T: 48627148, Avg. loss: 99070456378995738339704832.000000\n",
      "Total training time: 34.54 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 11652713283494.42, NNZs: 155839, Bias: 184753029.790599, T: 49813176, Avg. loss: 100355189238887811058237440.000000\n",
      "Total training time: 35.29 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 11809769856343.07, NNZs: 155839, Bias: -103187965.145991, T: 50999204, Avg. loss: 96444379928783691439931392.000000\n",
      "Total training time: 36.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 11486548697442.69, NNZs: 155839, Bias: -74221212.306788, T: 52185232, Avg. loss: 97801955106513930420224000.000000\n",
      "Total training time: 36.90 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 11653775426595.83, NNZs: 155839, Bias: 178887978.968138, T: 53371260, Avg. loss: 93975337949705927017889792.000000\n",
      "Total training time: 37.67 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 11368730575531.74, NNZs: 155839, Bias: 191550682.200420, T: 54557288, Avg. loss: 96079280143547740608004096.000000\n",
      "Total training time: 38.70 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 11494280768155.74, NNZs: 155839, Bias: 18466409.905103, T: 55743316, Avg. loss: 91968703486541150772264960.000000\n",
      "Total training time: 39.49 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 11237979023455.84, NNZs: 155839, Bias: 86093250.417648, T: 56929344, Avg. loss: 93218924692248641448968192.000000\n",
      "Total training time: 40.26 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 11375785351885.70, NNZs: 155839, Bias: -141995608.806549, T: 58115372, Avg. loss: 90544637070546335944409088.000000\n",
      "Total training time: 41.08 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 11161150937757.37, NNZs: 155839, Bias: -13317543.268390, T: 59301400, Avg. loss: 91459567239606612730576896.000000\n",
      "Total training time: 41.84 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 11239126851796.56, NNZs: 155839, Bias: -85155646.341901, T: 60487428, Avg. loss: 88718012609402749524639744.000000\n",
      "Total training time: 42.81 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 11013816696475.72, NNZs: 155839, Bias: 80343111.665806, T: 61673456, Avg. loss: 89541428778025559089217536.000000\n",
      "Total training time: 43.61 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 11149396046858.45, NNZs: 155839, Bias: 10470494.692087, T: 62859484, Avg. loss: 86357227897660049159356416.000000\n",
      "Total training time: 44.40 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 10913591538609.97, NNZs: 155839, Bias: -180448864.709903, T: 64045512, Avg. loss: 87688237734918545635016704.000000\n",
      "Total training time: 45.17 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 11064323409716.92, NNZs: 155839, Bias: 114282694.671721, T: 65231540, Avg. loss: 84635203340772215275651072.000000\n",
      "Total training time: 45.93 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 10804053089679.59, NNZs: 155839, Bias: -79110833.669170, T: 66417568, Avg. loss: 85991043658099987268501504.000000\n",
      "Total training time: 46.70 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 10915910173216.40, NNZs: 155839, Bias: -89620550.962079, T: 67603596, Avg. loss: 83415092708325820693741568.000000\n",
      "Total training time: 47.49 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 10681155534149.90, NNZs: 155839, Bias: -5432171.944039, T: 68789624, Avg. loss: 83976054627235455102353408.000000\n",
      "Total training time: 48.28 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 10818272771527.82, NNZs: 155839, Bias: -116320331.772118, T: 69975652, Avg. loss: 81824258293195585060077568.000000\n",
      "Total training time: 49.54 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 10563585111321.31, NNZs: 155839, Bias: -33527877.848105, T: 71161680, Avg. loss: 82737512713175272490270720.000000\n",
      "Total training time: 50.35 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 10740070597816.87, NNZs: 155839, Bias: -245624392.098614, T: 72347708, Avg. loss: 80287289611040689932468224.000000\n",
      "Total training time: 51.13 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 10509747060146.66, NNZs: 155839, Bias: -4788366.856597, T: 73533736, Avg. loss: 81383234232269825554513920.000000\n",
      "Total training time: 51.90 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 10609147245387.71, NNZs: 155839, Bias: -47666623.288170, T: 74719764, Avg. loss: 79391556577196277221031936.000000\n",
      "Total training time: 52.68 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 10426743525063.26, NNZs: 155839, Bias: -35505582.784931, T: 75905792, Avg. loss: 80239459677955097502941184.000000\n",
      "Total training time: 53.47 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 10541361500425.15, NNZs: 155839, Bias: 13823202.359459, T: 77091820, Avg. loss: 77949379760136624210444288.000000\n",
      "Total training time: 54.40 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 10372712352743.77, NNZs: 155839, Bias: -122132095.216739, T: 78277848, Avg. loss: 78550712408939311424077824.000000\n",
      "Total training time: 55.20 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 10472978699796.16, NNZs: 155839, Bias: -152204411.241304, T: 79463876, Avg. loss: 76605180537359461871779840.000000\n",
      "Total training time: 56.00 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 10281033164694.56, NNZs: 155839, Bias: -132274598.841330, T: 80649904, Avg. loss: 77857878401165305128681472.000000\n",
      "Total training time: 56.79 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 10378401270448.90, NNZs: 155839, Bias: 2244369.317371, T: 81835932, Avg. loss: 75523683410761912273600512.000000\n",
      "Total training time: 57.57 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 10187798747543.10, NNZs: 155839, Bias: -113877485.288608, T: 83021960, Avg. loss: 76164967011895042029649920.000000\n",
      "Total training time: 58.46 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 10292647916667.47, NNZs: 155839, Bias: 2083945.210644, T: 84207988, Avg. loss: 74553275523625871701180416.000000\n",
      "Total training time: 59.36 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 10127169110054.59, NNZs: 155839, Bias: 77835567.690597, T: 85394016, Avg. loss: 75212908284369560286527488.000000\n",
      "Total training time: 60.46 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 10186237905384.91, NNZs: 155839, Bias: 80047228.360343, T: 86580044, Avg. loss: 73564207389200952443535360.000000\n",
      "Total training time: 61.42 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 10054082661460.41, NNZs: 155839, Bias: 11082079.697386, T: 87766072, Avg. loss: 73717350375603667709460480.000000\n",
      "Total training time: 62.37 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 10150828835685.91, NNZs: 155839, Bias: -6238942.933488, T: 88952100, Avg. loss: 71996839025790482328322048.000000\n",
      "Total training time: 63.18 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 9962434395015.62, NNZs: 155839, Bias: -85223156.907894, T: 90138128, Avg. loss: 73124987798018980537434112.000000\n",
      "Total training time: 63.96 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 10063080365129.45, NNZs: 155839, Bias: -89959343.829249, T: 91324156, Avg. loss: 71408338936766823766425600.000000\n",
      "Total training time: 64.74 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 9923491636812.40, NNZs: 155839, Bias: 3478734.748342, T: 92510184, Avg. loss: 72080721425082622953914368.000000\n",
      "Total training time: 65.58 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 9990128223082.54, NNZs: 155839, Bias: -171229697.089898, T: 93696212, Avg. loss: 70169880865458006049947648.000000\n",
      "Total training time: 66.39 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 9817932863801.18, NNZs: 155839, Bias: -51734854.623364, T: 94882240, Avg. loss: 70860719538416786536923136.000000\n",
      "Total training time: 67.18 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 9930409666423.80, NNZs: 155839, Bias: 19258283.259080, T: 96068268, Avg. loss: 69552453973019009921056768.000000\n",
      "Total training time: 67.95 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 9774878244684.30, NNZs: 155839, Bias: -60599649.164382, T: 97254296, Avg. loss: 70020003313515611505557504.000000\n",
      "Total training time: 68.73 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 9846758323394.97, NNZs: 155839, Bias: -8525636.588560, T: 98440324, Avg. loss: 68272938333937243592327168.000000\n",
      "Total training time: 69.64 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 9719101852256.03, NNZs: 155839, Bias: -57081058.730977, T: 99626352, Avg. loss: 69251752132176925406789632.000000\n",
      "Total training time: 70.45 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 9802636870762.71, NNZs: 155839, Bias: 592417.258403, T: 100812380, Avg. loss: 67569684586821200394584064.000000\n",
      "Total training time: 71.32 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 9672600739019.32, NNZs: 155839, Bias: 120725228.287594, T: 101998408, Avg. loss: 68326632051610296227201024.000000\n",
      "Total training time: 72.21 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 9742888990140.72, NNZs: 155839, Bias: -129732033.915080, T: 103184436, Avg. loss: 66963717465831810250309632.000000\n",
      "Total training time: 73.03 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 9625721280716.08, NNZs: 155839, Bias: -48327964.435193, T: 104370464, Avg. loss: 67323579974658394708508672.000000\n",
      "Total training time: 73.85 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 9669679640893.22, NNZs: 155839, Bias: -37235802.486382, T: 105556492, Avg. loss: 66325834032443751462338560.000000\n",
      "Total training time: 74.61 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 9533888870140.41, NNZs: 155839, Bias: 52657084.405276, T: 106742520, Avg. loss: 66666895951342545078845440.000000\n",
      "Total training time: 75.38 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 9596166757552.87, NNZs: 155839, Bias: -18355080.541980, T: 107928548, Avg. loss: 65479095969678947359129600.000000\n",
      "Total training time: 76.13 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 9492262933766.06, NNZs: 155839, Bias: -54155094.813465, T: 109114576, Avg. loss: 65832100472392377037750272.000000\n",
      "Total training time: 76.88 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 9547603499589.90, NNZs: 155839, Bias: 7464716.131910, T: 110300604, Avg. loss: 64489062014828602057555968.000000\n",
      "Total training time: 77.66 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 9434757493965.94, NNZs: 155839, Bias: -1441682.144569, T: 111486632, Avg. loss: 65317404219300511668502528.000000\n",
      "Total training time: 78.59 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 9521160586037.93, NNZs: 155839, Bias: 71174645.658716, T: 112672660, Avg. loss: 63531398811865394517639168.000000\n",
      "Total training time: 79.36 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 9358811549474.84, NNZs: 155839, Bias: 1404741.760420, T: 113858688, Avg. loss: 64608292991340368533389312.000000\n",
      "Total training time: 80.11 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 9471473820737.74, NNZs: 155839, Bias: 9528985.618498, T: 115044716, Avg. loss: 62892142468747784813019136.000000\n",
      "Total training time: 80.87 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 9343910780343.38, NNZs: 155839, Bias: -69111469.206691, T: 116230744, Avg. loss: 64065251121968589087703040.000000\n",
      "Total training time: 81.62 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 9398562450622.79, NNZs: 155839, Bias: -22058743.943421, T: 117416772, Avg. loss: 62545480377483095967793152.000000\n",
      "Total training time: 82.47 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 9253848975399.94, NNZs: 155839, Bias: 11270911.813931, T: 118602800, Avg. loss: 63170863224595531893309440.000000\n",
      "Total training time: 83.43 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 9362002026983.61, NNZs: 155839, Bias: -29828725.957015, T: 119788828, Avg. loss: 61673149300316946483380224.000000\n",
      "Total training time: 84.24 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 9276803578876.08, NNZs: 155839, Bias: -78004517.936918, T: 120974856, Avg. loss: 62338104013599666354520064.000000\n",
      "Total training time: 84.99 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 9305707518417.06, NNZs: 155839, Bias: 122742787.384755, T: 122160884, Avg. loss: 61582297575732365850836992.000000\n",
      "Total training time: 85.74 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 9184207851662.69, NNZs: 155839, Bias: 12288202.895048, T: 123346912, Avg. loss: 61831636330374275081961472.000000\n",
      "Total training time: 86.49 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 9285247873226.65, NNZs: 155839, Bias: -64632546.864686, T: 124532940, Avg. loss: 60526137923469122820112384.000000\n",
      "Total training time: 87.24 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 9176950134844.18, NNZs: 155839, Bias: -14830391.560568, T: 125718968, Avg. loss: 61156106367552905929555968.000000\n",
      "Total training time: 88.00 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 9209908779643.30, NNZs: 155839, Bias: 139885391.642886, T: 126904996, Avg. loss: 60018358062735687612891136.000000\n",
      "Total training time: 88.75 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 9113785005884.89, NNZs: 155839, Bias: -208245396.712717, T: 128091024, Avg. loss: 60259801201285305691275264.000000\n",
      "Total training time: 89.57 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 9181418415339.05, NNZs: 155839, Bias: -213119852.783581, T: 129277052, Avg. loss: 59237597058580911937814528.000000\n",
      "Total training time: 90.33 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 9054661561775.09, NNZs: 155839, Bias: -102074218.171337, T: 130463080, Avg. loss: 59790931197981693524836352.000000\n",
      "Total training time: 91.09 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 9147883105874.68, NNZs: 155839, Bias: 18287850.833317, T: 131649108, Avg. loss: 58797263816370678182969344.000000\n",
      "Total training time: 91.84 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 9045455867295.13, NNZs: 155839, Bias: 25222968.813201, T: 132835136, Avg. loss: 59185010170718608387211264.000000\n",
      "Total training time: 92.60 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 9092033063886.97, NNZs: 155839, Bias: -25649894.615222, T: 134021164, Avg. loss: 58188349026079792322052096.000000\n",
      "Total training time: 93.38 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 8958028237886.47, NNZs: 155839, Bias: 3538990.386043, T: 135207192, Avg. loss: 58768752847483247230189568.000000\n",
      "Total training time: 94.18 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 9002383773849.30, NNZs: 155839, Bias: 18977003.520300, T: 136393220, Avg. loss: 57921422849493778770690048.000000\n",
      "Total training time: 95.05 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 8923877759110.59, NNZs: 155839, Bias: 31939796.298434, T: 137579248, Avg. loss: 58262709306563130896154624.000000\n",
      "Total training time: 95.83 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 8980640219713.07, NNZs: 155839, Bias: -114265541.115831, T: 138765276, Avg. loss: 57336183571622919604797440.000000\n",
      "Total training time: 96.63 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 8890889761165.04, NNZs: 155839, Bias: -57224757.771871, T: 139951304, Avg. loss: 57550626540095004508946432.000000\n",
      "Total training time: 97.40 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 8950467082990.27, NNZs: 155839, Bias: -108239956.746856, T: 141137332, Avg. loss: 56976847153632823560110080.000000\n",
      "Total training time: 98.20 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 8847023767035.54, NNZs: 155839, Bias: -52547289.018849, T: 142323360, Avg. loss: 57013909514312676374740992.000000\n",
      "Total training time: 99.00 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 8909279326639.95, NNZs: 155839, Bias: -79396420.504262, T: 143509388, Avg. loss: 56436384565976894059577344.000000\n",
      "Total training time: 99.76 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 8799868943936.17, NNZs: 155839, Bias: -3457172.383430, T: 144695416, Avg. loss: 56906140983182538875863040.000000\n",
      "Total training time: 100.51 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 8851162864133.72, NNZs: 155839, Bias: -47783485.110200, T: 145881444, Avg. loss: 55704235062685363502841856.000000\n",
      "Total training time: 101.25 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 8775414880638.28, NNZs: 155839, Bias: 62514977.454392, T: 147067472, Avg. loss: 56212368458352054184706048.000000\n",
      "Total training time: 102.01 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 8822672818121.97, NNZs: 155839, Bias: 52235557.895987, T: 148253500, Avg. loss: 55407011520141401398444032.000000\n",
      "Total training time: 102.76 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 8741226966854.78, NNZs: 155839, Bias: -230275274.749782, T: 149439528, Avg. loss: 55625140774645508511629312.000000\n",
      "Total training time: 103.54 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 8783690031134.13, NNZs: 155839, Bias: -138973702.840360, T: 150625556, Avg. loss: 54959000100560444697083904.000000\n",
      "Total training time: 104.31 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 8695256683428.21, NNZs: 155839, Bias: -155064460.313143, T: 151811584, Avg. loss: 55150542129064401046601728.000000\n",
      "Total training time: 105.07 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 8755337817476.08, NNZs: 155839, Bias: -101070379.181187, T: 152997612, Avg. loss: 54197000453633869084622848.000000\n",
      "Total training time: 105.83 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 8647957373410.19, NNZs: 155839, Bias: -94142159.413400, T: 154183640, Avg. loss: 54781052915100239873966080.000000\n",
      "Total training time: 106.59 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 8724881739662.62, NNZs: 155839, Bias: -104407261.687944, T: 155369668, Avg. loss: 53842139238561877770371072.000000\n",
      "Total training time: 107.36 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 8623530969494.12, NNZs: 155839, Bias: 50522240.507647, T: 156555696, Avg. loss: 54404969728928252080160768.000000\n",
      "Total training time: 108.17 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 8670751972695.06, NNZs: 155839, Bias: -8272271.658306, T: 157741724, Avg. loss: 53864259499241243043430400.000000\n",
      "Total training time: 108.96 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 8591459779092.91, NNZs: 155839, Bias: 23735991.628506, T: 158927752, Avg. loss: 54132368768574745212878848.000000\n",
      "Total training time: 109.74 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 8635692212091.98, NNZs: 155839, Bias: -121904921.622042, T: 160113780, Avg. loss: 53365565455377243962343424.000000\n",
      "Total training time: 110.48 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 8548363269397.39, NNZs: 155839, Bias: -25538368.367296, T: 161299808, Avg. loss: 53700680212909750703095808.000000\n",
      "Total training time: 111.23 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 8601145068170.72, NNZs: 155839, Bias: -95171568.482287, T: 162485836, Avg. loss: 52736717537653967969845248.000000\n",
      "Total training time: 111.98 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 8526946918202.87, NNZs: 155839, Bias: 12715998.517451, T: 163671864, Avg. loss: 53079956311286271130468352.000000\n",
      "Total training time: 112.74 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 8560517743891.95, NNZs: 155839, Bias: -75098075.112957, T: 164857892, Avg. loss: 52433358223600995320987648.000000\n",
      "Total training time: 113.52 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 8490200368391.28, NNZs: 155839, Bias: 30257441.640368, T: 166043920, Avg. loss: 52654395433137527339352064.000000\n",
      "Total training time: 114.29 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 8531252089254.99, NNZs: 155839, Bias: 36482379.305875, T: 167229948, Avg. loss: 52028092953169090238742528.000000\n",
      "Total training time: 115.04 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 8456258014949.50, NNZs: 155839, Bias: -63192535.150685, T: 168415976, Avg. loss: 52209588705772030220304384.000000\n",
      "Total training time: 115.80 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 8500990504926.57, NNZs: 155839, Bias: -3932423.509412, T: 169602004, Avg. loss: 51600486579450811330330624.000000\n",
      "Total training time: 116.56 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 8428025331798.69, NNZs: 155839, Bias: -40357881.593310, T: 170788032, Avg. loss: 52230134510909671497269248.000000\n",
      "Total training time: 117.31 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 8477274444204.07, NNZs: 155839, Bias: -81139880.213152, T: 171974060, Avg. loss: 51199679027931864079794176.000000\n",
      "Total training time: 118.06 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 8407526606521.42, NNZs: 155839, Bias: 130532375.068200, T: 173160088, Avg. loss: 51490623708503699762446336.000000\n",
      "Total training time: 118.85 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 8462596049763.80, NNZs: 155839, Bias: 147207100.998603, T: 174346116, Avg. loss: 50791159906110610708889600.000000\n",
      "Total training time: 119.61 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 8370509281253.23, NNZs: 155839, Bias: 122240104.139068, T: 175532144, Avg. loss: 51479662810645890720595968.000000\n",
      "Total training time: 120.38 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 8415706618830.07, NNZs: 155839, Bias: 95792292.229830, T: 176718172, Avg. loss: 50428797739656385073774592.000000\n",
      "Total training time: 121.13 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 8347046464318.52, NNZs: 155839, Bias: 17665598.954049, T: 177904200, Avg. loss: 50629674588043797420048384.000000\n",
      "Total training time: 121.88 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 8383954795646.20, NNZs: 155839, Bias: 4636277.668026, T: 179090228, Avg. loss: 50314704073495184733634560.000000\n",
      "Total training time: 122.63 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 8305590962944.21, NNZs: 155839, Bias: 95028810.284949, T: 180276256, Avg. loss: 50424279254152062949130240.000000\n",
      "Total training time: 123.39 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 8349940379728.38, NNZs: 155839, Bias: 1633573.841170, T: 181462284, Avg. loss: 49778529845756965771280384.000000\n",
      "Total training time: 124.15 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 8294087292157.99, NNZs: 155839, Bias: -36539369.696112, T: 182648312, Avg. loss: 49986787539613682986123264.000000\n",
      "Total training time: 124.90 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 8346061422493.38, NNZs: 155839, Bias: -2253436.286928, T: 183834340, Avg. loss: 49364189191887847562412032.000000\n",
      "Total training time: 125.66 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 8270234388833.25, NNZs: 155839, Bias: 4038897.481301, T: 185020368, Avg. loss: 49636852058400706149744640.000000\n",
      "Total training time: 126.40 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 8291390450395.83, NNZs: 155839, Bias: -81409019.198252, T: 186206396, Avg. loss: 49056857368794703182430208.000000\n",
      "Total training time: 127.15 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 8243890292284.93, NNZs: 155839, Bias: -54040634.881581, T: 187392424, Avg. loss: 49554353242459668140261376.000000\n",
      "Total training time: 127.89 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 8255284103588.72, NNZs: 155839, Bias: -41794023.500178, T: 188578452, Avg. loss: 48914048412643284742045696.000000\n",
      "Total training time: 128.68 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 8204835013487.85, NNZs: 155839, Bias: -46997310.675857, T: 189764480, Avg. loss: 49101280329269399699587072.000000\n",
      "Total training time: 129.48 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 8230081845445.88, NNZs: 155839, Bias: 98422000.908573, T: 190950508, Avg. loss: 48561614631594961758322688.000000\n",
      "Total training time: 130.26 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 8179178736538.74, NNZs: 155839, Bias: 6307027.146327, T: 192136536, Avg. loss: 48770870945982306603499520.000000\n",
      "Total training time: 131.01 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 8205486181222.89, NNZs: 155839, Bias: -136431594.305634, T: 193322564, Avg. loss: 48103344701584764557066240.000000\n",
      "Total training time: 131.76 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 8144720100408.23, NNZs: 155839, Bias: -76248448.704072, T: 194508592, Avg. loss: 48539372194725265246519296.000000\n",
      "Total training time: 132.51 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 8195072875429.07, NNZs: 155839, Bias: 22833292.244528, T: 195694620, Avg. loss: 47947542428639164241018880.000000\n",
      "Total training time: 133.28 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 8121329033126.23, NNZs: 155839, Bias: -70880905.174495, T: 196880648, Avg. loss: 48149989050683910741032960.000000\n",
      "Total training time: 134.07 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 8167606737774.46, NNZs: 155839, Bias: 9132583.515646, T: 198066676, Avg. loss: 47660738385636012173819904.000000\n",
      "Total training time: 134.83 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 8094085122516.48, NNZs: 155839, Bias: -58471515.667815, T: 199252704, Avg. loss: 47805233541921067856035840.000000\n",
      "Total training time: 135.59 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 8097784986770.24, NNZs: 155839, Bias: -18438340.637959, T: 200438732, Avg. loss: 47320131560647475154911232.000000\n",
      "Total training time: 136.33 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 8064953515924.28, NNZs: 155839, Bias: -50192671.866562, T: 201624760, Avg. loss: 47567348883146060966395904.000000\n",
      "Total training time: 137.08 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 8097459315082.72, NNZs: 155839, Bias: -113839838.163164, T: 202810788, Avg. loss: 46948295342950453245640704.000000\n",
      "Total training time: 137.83 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 8028193048144.12, NNZs: 155839, Bias: -41969749.991219, T: 203996816, Avg. loss: 47403587868884739502899200.000000\n",
      "Total training time: 138.61 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 8069903500665.32, NNZs: 155839, Bias: -56832324.394401, T: 205182844, Avg. loss: 46660411389955411151945728.000000\n",
      "Total training time: 139.42 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 8028272893662.59, NNZs: 155839, Bias: 5196653.537369, T: 206368872, Avg. loss: 47085370209126332458074112.000000\n",
      "Total training time: 140.16 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 8068814777914.94, NNZs: 155839, Bias: -27485364.054219, T: 207554900, Avg. loss: 46315950619234352911876096.000000\n",
      "Total training time: 140.93 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 8024147987892.51, NNZs: 155839, Bias: -32183603.884433, T: 208740928, Avg. loss: 46685226174927818610180096.000000\n",
      "Total training time: 141.68 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 8063998009092.75, NNZs: 155839, Bias: 14213619.983534, T: 209926956, Avg. loss: 46326383146587809936048128.000000\n",
      "Total training time: 142.42 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 8010257809578.79, NNZs: 155839, Bias: -31741753.904673, T: 211112984, Avg. loss: 46431512315522344164524032.000000\n",
      "Total training time: 143.19 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 8009521789786.64, NNZs: 155839, Bias: 13834154.791676, T: 212299012, Avg. loss: 45942492647534772222951424.000000\n",
      "Total training time: 143.95 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 7962458187298.09, NNZs: 155839, Bias: 24285169.128371, T: 213485040, Avg. loss: 45946453629479644634808320.000000\n",
      "Total training time: 144.73 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 7989507934691.53, NNZs: 155839, Bias: 12103430.309752, T: 214671068, Avg. loss: 45654654598498522979368960.000000\n",
      "Total training time: 145.64 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 7906656478725.00, NNZs: 155839, Bias: -55569449.768064, T: 215857096, Avg. loss: 45937722249800566842589184.000000\n",
      "Total training time: 146.45 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 7966373839980.49, NNZs: 155839, Bias: -50623857.634947, T: 217043124, Avg. loss: 45475014335202103325097984.000000\n",
      "Total training time: 147.21 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 7886641870000.56, NNZs: 155839, Bias: 41493911.220623, T: 218229152, Avg. loss: 45813241038916690544427008.000000\n",
      "Total training time: 147.96 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 7959666432529.98, NNZs: 155839, Bias: -65883670.585855, T: 219415180, Avg. loss: 45207999677188118350397440.000000\n",
      "Total training time: 148.74 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 7897144739670.20, NNZs: 155839, Bias: -134036108.336921, T: 220601208, Avg. loss: 45513921104717506179760128.000000\n",
      "Total training time: 149.55 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 7897658888035.84, NNZs: 155839, Bias: 60844259.591276, T: 221787236, Avg. loss: 44846615370028051893059584.000000\n",
      "Total training time: 150.36 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 7843140672304.40, NNZs: 155839, Bias: -108443113.574286, T: 222973264, Avg. loss: 45138530585269636337500160.000000\n",
      "Total training time: 151.11 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 7862461058344.79, NNZs: 155839, Bias: -27104083.112743, T: 224159292, Avg. loss: 44693344038139351439245312.000000\n",
      "Total training time: 151.86 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 7813590160756.19, NNZs: 155839, Bias: 83022412.038717, T: 225345320, Avg. loss: 44638993222076142559690752.000000\n",
      "Total training time: 152.61 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 7872285740814.19, NNZs: 155839, Bias: -42714595.535924, T: 226531348, Avg. loss: 44200904083995658807148544.000000\n",
      "Total training time: 153.38 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 7797465480976.37, NNZs: 155839, Bias: -99886856.403118, T: 227717376, Avg. loss: 44591205681944970139795456.000000\n",
      "Total training time: 154.15 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 7848044796154.39, NNZs: 155839, Bias: -46728140.547045, T: 228903404, Avg. loss: 44145455119692082983206912.000000\n",
      "Total training time: 154.92 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 7781951292295.78, NNZs: 155839, Bias: -2807165.334939, T: 230089432, Avg. loss: 44334002941779841017446400.000000\n",
      "Total training time: 155.69 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 7828532701796.21, NNZs: 155839, Bias: -43975905.594602, T: 231275460, Avg. loss: 44201895939699014209896448.000000\n",
      "Total training time: 156.44 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 7772604755733.07, NNZs: 155839, Bias: 16673130.457055, T: 232461488, Avg. loss: 44257677063682554542948352.000000\n",
      "Total training time: 157.20 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 7796087669442.90, NNZs: 155839, Bias: -80212173.372779, T: 233647516, Avg. loss: 43625673033536283120500736.000000\n",
      "Total training time: 157.96 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 7749932001987.05, NNZs: 155839, Bias: -9026687.909045, T: 234833544, Avg. loss: 43842372969073902597701632.000000\n",
      "Total training time: 158.72 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 7757650187570.11, NNZs: 155839, Bias: 66447094.293477, T: 236019572, Avg. loss: 43452067292728177897504768.000000\n",
      "Total training time: 159.49 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 7709810018907.78, NNZs: 155839, Bias: 4086012.170180, T: 237205600, Avg. loss: 43462327316497769437331456.000000\n",
      "Total training time: 160.26 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 7771598469154.92, NNZs: 155839, Bias: 91594830.944741, T: 238391628, Avg. loss: 43078463921552760483348480.000000\n",
      "Total training time: 161.00 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 7708055984343.45, NNZs: 155839, Bias: -54850573.232538, T: 239577656, Avg. loss: 43425868949397898562371584.000000\n",
      "Total training time: 161.74 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 7746411237054.45, NNZs: 155839, Bias: 78906332.457004, T: 240763684, Avg. loss: 42960633584045895800848384.000000\n",
      "Total training time: 162.48 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 7692577041727.70, NNZs: 155839, Bias: -36936169.021385, T: 241949712, Avg. loss: 43195544745751241973301248.000000\n",
      "Total training time: 163.24 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 7707574280054.92, NNZs: 155839, Bias: 41909778.650522, T: 243135740, Avg. loss: 42916762795286710766796800.000000\n",
      "Total training time: 163.99 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 7668699986637.19, NNZs: 155839, Bias: -67753987.963242, T: 244321768, Avg. loss: 42897469128090651625259008.000000\n",
      "Total training time: 164.75 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 7705681564438.59, NNZs: 155839, Bias: 22067301.308655, T: 245507796, Avg. loss: 42535952087094965389230080.000000\n",
      "Total training time: 165.52 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 7640458221491.76, NNZs: 155839, Bias: -66268573.962752, T: 246693824, Avg. loss: 42705271612893118618140672.000000\n",
      "Total training time: 166.27 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 7667660809359.50, NNZs: 155839, Bias: -71534261.321137, T: 247879852, Avg. loss: 42294475816466970771980288.000000\n",
      "Total training time: 167.02 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 7624687152356.94, NNZs: 155839, Bias: 2861812.107198, T: 249065880, Avg. loss: 42484034051377122299084800.000000\n",
      "Total training time: 167.82 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 7655258870321.04, NNZs: 155839, Bias: -32977507.938384, T: 250251908, Avg. loss: 42057860262154223908552704.000000\n",
      "Total training time: 168.58 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 7618146460285.11, NNZs: 155839, Bias: -6215680.718689, T: 251437936, Avg. loss: 42346417566211487293243392.000000\n",
      "Total training time: 169.34 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 7648450884650.91, NNZs: 155839, Bias: -29798365.459416, T: 252623964, Avg. loss: 41867533327640526766211072.000000\n",
      "Total training time: 170.10 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 7602498813593.37, NNZs: 155839, Bias: -140039990.743102, T: 253809992, Avg. loss: 42073389391052270091632640.000000\n",
      "Total training time: 170.87 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 7625977295786.87, NNZs: 155839, Bias: -111553944.477704, T: 254996020, Avg. loss: 41994178863541084024209408.000000\n",
      "Total training time: 171.63 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 7567793329112.25, NNZs: 155839, Bias: -38702901.977697, T: 256182048, Avg. loss: 41945054620376832017432576.000000\n",
      "Total training time: 172.38 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 7573839445951.24, NNZs: 155839, Bias: 4523252.042168, T: 257368076, Avg. loss: 41424643877248698232602624.000000\n",
      "Total training time: 173.14 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 7562481784564.14, NNZs: 155839, Bias: -21550727.017341, T: 258554104, Avg. loss: 41620882974021270452043776.000000\n",
      "Total training time: 173.90 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 7541497011755.78, NNZs: 155839, Bias: 17297757.900316, T: 259740132, Avg. loss: 41461282891417485548978176.000000\n",
      "Total training time: 174.66 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 7529157424795.37, NNZs: 155839, Bias: 66083690.363824, T: 260926160, Avg. loss: 41737180956833370838401024.000000\n",
      "Total training time: 175.42 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 7548367182786.69, NNZs: 155839, Bias: 13228723.108764, T: 262112188, Avg. loss: 41192377088657474004713472.000000\n",
      "Total training time: 176.19 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 7519652578963.87, NNZs: 155839, Bias: -903033.973359, T: 263298216, Avg. loss: 41224733289314478001750016.000000\n",
      "Total training time: 176.94 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 7532144768114.03, NNZs: 155839, Bias: -60772041.993330, T: 264484244, Avg. loss: 40907492688009099114184704.000000\n",
      "Total training time: 177.70 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 7490756806687.16, NNZs: 155839, Bias: 2029883.958309, T: 265670272, Avg. loss: 41353909343636158079303680.000000\n",
      "Total training time: 178.45 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 7525720806374.45, NNZs: 155839, Bias: 5996361.895368, T: 266856300, Avg. loss: 40726769107800825842892800.000000\n",
      "Total training time: 179.22 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 7483802236077.99, NNZs: 155839, Bias: -62156837.695529, T: 268042328, Avg. loss: 40921641412776287849676800.000000\n",
      "Total training time: 179.96 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 7500418823541.81, NNZs: 155839, Bias: -148517945.921520, T: 269228356, Avg. loss: 40497063318274236936617984.000000\n",
      "Total training time: 180.71 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 7507099237430.21, NNZs: 155839, Bias: -80452038.614323, T: 270414384, Avg. loss: 40478186858356719738683392.000000\n",
      "Total training time: 181.47 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 7500629731548.31, NNZs: 155839, Bias: 3540683.955897, T: 271600412, Avg. loss: 40469554869462561143652352.000000\n",
      "Total training time: 182.22 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 7462025604854.75, NNZs: 155839, Bias: 15571699.476464, T: 272786440, Avg. loss: 40466801065065743105654784.000000\n",
      "Total training time: 182.98 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 7503830506884.36, NNZs: 155839, Bias: 115067527.047894, T: 273972468, Avg. loss: 40342507253330815880265728.000000\n",
      "Total training time: 183.73 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 7432526158618.99, NNZs: 155839, Bias: -21031430.244557, T: 275158496, Avg. loss: 40446668244240944026615808.000000\n",
      "Total training time: 184.49 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 7426762257837.59, NNZs: 155839, Bias: 51767470.436774, T: 276344524, Avg. loss: 40083172323118323337265152.000000\n",
      "Total training time: 185.22 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 7395402724840.36, NNZs: 155839, Bias: 30529319.786788, T: 277530552, Avg. loss: 39995054304654541042221056.000000\n",
      "Total training time: 185.98 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 7437135645202.11, NNZs: 155839, Bias: 17343346.719333, T: 278716580, Avg. loss: 39629301827213530138411008.000000\n",
      "Total training time: 186.73 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 7410577201786.33, NNZs: 155839, Bias: -136983631.901220, T: 279902608, Avg. loss: 40039771155986403785965568.000000\n",
      "Total training time: 187.47 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 7419867677738.83, NNZs: 155839, Bias: 32399854.245628, T: 281088636, Avg. loss: 39631910140863140696424448.000000\n",
      "Total training time: 188.22 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 7382379312764.17, NNZs: 155839, Bias: 2325756.524989, T: 282274664, Avg. loss: 39856749551327038861737984.000000\n",
      "Total training time: 188.97 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 7404052566664.03, NNZs: 155839, Bias: 231589209.040798, T: 283460692, Avg. loss: 39457372075764824319983616.000000\n",
      "Total training time: 189.72 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 7389537704876.27, NNZs: 155839, Bias: -80342970.987204, T: 284646720, Avg. loss: 39451778498360373611069440.000000\n",
      "Total training time: 190.46 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 7375927317647.95, NNZs: 155839, Bias: 6466872.129126, T: 285832748, Avg. loss: 39493417569513982356619264.000000\n",
      "Total training time: 191.24 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 7350940388279.76, NNZs: 155839, Bias: 4112256.324327, T: 287018776, Avg. loss: 39426226862192343266623488.000000\n",
      "Total training time: 192.00 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 7382704004468.42, NNZs: 155839, Bias: 66965683.514133, T: 288204804, Avg. loss: 39219022194411551368151040.000000\n",
      "Total training time: 192.74 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 7331778001037.91, NNZs: 155839, Bias: -72361141.022220, T: 289390832, Avg. loss: 39235495924918331474182144.000000\n",
      "Total training time: 193.50 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 7358687079486.61, NNZs: 155839, Bias: 12962963.835752, T: 290576860, Avg. loss: 38991987540890017540866048.000000\n",
      "Total training time: 194.24 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 7327457371122.96, NNZs: 155839, Bias: -12376490.221204, T: 291762888, Avg. loss: 39167530689950563090563072.000000\n",
      "Total training time: 194.98 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 7341163465508.96, NNZs: 155839, Bias: -10392150.590530, T: 292948916, Avg. loss: 38870260644329497028460544.000000\n",
      "Total training time: 195.73 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 7338881860867.00, NNZs: 155839, Bias: 21094452.547558, T: 294134944, Avg. loss: 38793123500247889332928512.000000\n",
      "Total training time: 196.50 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 7343632434539.17, NNZs: 155839, Bias: -21230437.091380, T: 295320972, Avg. loss: 38728881064449394276827136.000000\n",
      "Total training time: 197.26 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 7300796449487.00, NNZs: 155839, Bias: 40211794.064870, T: 296507000, Avg. loss: 38783489603979653335220224.000000\n",
      "Total training time: 198.01 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 7326433974204.87, NNZs: 155839, Bias: 12662673.633033, T: 297693028, Avg. loss: 38582165636833347029172224.000000\n",
      "Total training time: 198.77 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 7292881166190.13, NNZs: 155839, Bias: 25905534.449588, T: 298879056, Avg. loss: 38829799989778824302690304.000000\n",
      "Total training time: 199.53 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 7306949848156.62, NNZs: 155839, Bias: 4220873.773432, T: 300065084, Avg. loss: 38389914152554211171106816.000000\n",
      "Total training time: 200.29 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 7280365651051.00, NNZs: 155839, Bias: 88912094.664261, T: 301251112, Avg. loss: 38376717305664100651499520.000000\n",
      "Total training time: 201.03 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 7269519708752.14, NNZs: 155839, Bias: -33013361.057121, T: 302437140, Avg. loss: 38214397495851137131085824.000000\n",
      "Total training time: 201.81 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 7276893920199.56, NNZs: 155839, Bias: -67540547.071730, T: 303623168, Avg. loss: 38186172305280197527076864.000000\n",
      "Total training time: 202.55 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 7271603800301.73, NNZs: 155839, Bias: -104669274.380951, T: 304809196, Avg. loss: 37977788961012207595814912.000000\n",
      "Total training time: 203.30 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 7244620099783.72, NNZs: 155839, Bias: -62548165.787989, T: 305995224, Avg. loss: 38189928622484341735292928.000000\n",
      "Total training time: 204.06 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 7278051159732.12, NNZs: 155839, Bias: -70615414.360334, T: 307181252, Avg. loss: 38109348356373188096032768.000000\n",
      "Total training time: 204.81 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 7242772649270.99, NNZs: 155839, Bias: 47434233.285996, T: 308367280, Avg. loss: 37990439033010095357165568.000000\n",
      "Total training time: 205.57 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 7252875938886.63, NNZs: 155839, Bias: 8134841.646750, T: 309553308, Avg. loss: 37627262968394819603267584.000000\n",
      "Total training time: 206.32 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 7232438010861.19, NNZs: 155839, Bias: -40337468.131553, T: 310739336, Avg. loss: 37768852431568625081516032.000000\n",
      "Total training time: 207.09 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 7213634716504.62, NNZs: 155839, Bias: 58543652.490138, T: 311925364, Avg. loss: 37643780845068060857466880.000000\n",
      "Total training time: 207.84 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 7223444436674.69, NNZs: 155839, Bias: -59041545.992672, T: 313111392, Avg. loss: 37611988216379968604602368.000000\n",
      "Total training time: 208.65 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 7206167818007.70, NNZs: 155839, Bias: 22404661.480345, T: 314297420, Avg. loss: 37474965968183766554771456.000000\n",
      "Total training time: 209.41 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 7167618049174.97, NNZs: 155839, Bias: -75789544.218586, T: 315483448, Avg. loss: 37436800805954248683552768.000000\n",
      "Total training time: 210.21 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 7199360016819.07, NNZs: 155839, Bias: -73329816.373150, T: 316669476, Avg. loss: 37295618840168257187479552.000000\n",
      "Total training time: 211.08 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 7188932435394.68, NNZs: 155839, Bias: -31374588.973525, T: 317855504, Avg. loss: 37289033170002724765302784.000000\n",
      "Total training time: 211.90 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 7185922217572.04, NNZs: 155839, Bias: -44507913.461242, T: 319041532, Avg. loss: 37280698598481972332331008.000000\n",
      "Total training time: 212.70 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 7137938950552.45, NNZs: 155839, Bias: -29523897.573810, T: 320227560, Avg. loss: 37204970529257554498813952.000000\n",
      "Total training time: 213.52 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 7166864898516.25, NNZs: 155839, Bias: 79526254.557342, T: 321413588, Avg. loss: 36938467936774484953923584.000000\n",
      "Total training time: 214.32 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 7135618782389.19, NNZs: 155839, Bias: -58091950.692365, T: 322599616, Avg. loss: 37199843778618031618392064.000000\n",
      "Total training time: 215.10 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 7151089207837.25, NNZs: 155839, Bias: 18767512.902366, T: 323785644, Avg. loss: 36876612519956941648166912.000000\n",
      "Total training time: 215.87 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 7111260645587.37, NNZs: 155839, Bias: 78083236.845902, T: 324971672, Avg. loss: 36851453928027259261681664.000000\n",
      "Total training time: 216.66 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 7143883215161.70, NNZs: 155839, Bias: -641166.153775, T: 326157700, Avg. loss: 36542050545968148058734592.000000\n",
      "Total training time: 217.42 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 7105165898031.59, NNZs: 155839, Bias: -39458939.491260, T: 327343728, Avg. loss: 36893874720834588887744512.000000\n",
      "Total training time: 218.18 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 7131884560191.41, NNZs: 155839, Bias: -77263129.143502, T: 328529756, Avg. loss: 36561332164626836963196928.000000\n",
      "Total training time: 218.95 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 7101319025490.04, NNZs: 155839, Bias: -49187290.969104, T: 329715784, Avg. loss: 36606886604644838954500096.000000\n",
      "Total training time: 219.72 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 7120415188472.03, NNZs: 155839, Bias: 4082872.186694, T: 330901812, Avg. loss: 36471058745464892919119872.000000\n",
      "Total training time: 220.48 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 7079846940501.02, NNZs: 155839, Bias: 7031944.814064, T: 332087840, Avg. loss: 36456786402029326544404480.000000\n",
      "Total training time: 221.23 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 7104804677913.28, NNZs: 155839, Bias: 92695384.257393, T: 333273868, Avg. loss: 36356837034656028759687168.000000\n",
      "Total training time: 221.98 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 7072404680368.58, NNZs: 155839, Bias: -11285677.397457, T: 334459896, Avg. loss: 36340226804771992081793024.000000\n",
      "Total training time: 222.74 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 7079794885563.22, NNZs: 155839, Bias: 25403657.999942, T: 335645924, Avg. loss: 36108828453010017840267264.000000\n",
      "Total training time: 223.51 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 7056405147492.70, NNZs: 155839, Bias: -25308084.091630, T: 336831952, Avg. loss: 36241861031328267974475776.000000\n",
      "Total training time: 224.28 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 7026106711726.75, NNZs: 155839, Bias: -80388816.199243, T: 338017980, Avg. loss: 36101118294788595234373632.000000\n",
      "Total training time: 225.03 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 7048815619668.32, NNZs: 155839, Bias: 25421415.086397, T: 339204008, Avg. loss: 36089117474385474894168064.000000\n",
      "Total training time: 225.78 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 7034526666812.57, NNZs: 155839, Bias: -109773745.508663, T: 340390036, Avg. loss: 35937063691918037179432960.000000\n",
      "Total training time: 226.54 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 7030033279227.15, NNZs: 155839, Bias: 25122445.059601, T: 341576064, Avg. loss: 35772091209759129495142400.000000\n",
      "Total training time: 227.31 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 7037030658207.70, NNZs: 155839, Bias: -75541508.241575, T: 342762092, Avg. loss: 35721281630415678731190272.000000\n",
      "Total training time: 228.07 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 7030198108006.24, NNZs: 155839, Bias: 46877116.197920, T: 343948120, Avg. loss: 35821373300101236817657856.000000\n",
      "Total training time: 228.85 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 7023709480232.04, NNZs: 155839, Bias: 35697729.266876, T: 345134148, Avg. loss: 35609274656858018446049280.000000\n",
      "Total training time: 229.63 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 7002432579748.41, NNZs: 155839, Bias: 43164105.966049, T: 346320176, Avg. loss: 35757612899515112032829440.000000\n",
      "Total training time: 230.39 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 6995523283908.97, NNZs: 155839, Bias: 142336502.618186, T: 347506204, Avg. loss: 35595050161076971128225792.000000\n",
      "Total training time: 231.13 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 7004756888220.13, NNZs: 155839, Bias: 8779902.168403, T: 348692232, Avg. loss: 35603603727988539904753664.000000\n",
      "Total training time: 231.87 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 7006796763688.66, NNZs: 155839, Bias: -19299025.610778, T: 349878260, Avg. loss: 35457239862072647067107328.000000\n",
      "Total training time: 232.62 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 6971451066226.24, NNZs: 155839, Bias: -42000596.264193, T: 351064288, Avg. loss: 35559187031753374367219712.000000\n",
      "Total training time: 233.39 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 6975804717455.60, NNZs: 155839, Bias: -76361726.709634, T: 352250316, Avg. loss: 35162368613635607591124992.000000\n",
      "Total training time: 234.17 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 6939073549149.56, NNZs: 155839, Bias: 19594854.966258, T: 353436344, Avg. loss: 35220843810812110017921024.000000\n",
      "Total training time: 234.93 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 6945902959384.35, NNZs: 155839, Bias: -75450811.959383, T: 354622372, Avg. loss: 35117199475836433654087680.000000\n",
      "Total training time: 235.67 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 6942399568791.18, NNZs: 155839, Bias: 38494303.926682, T: 355808400, Avg. loss: 35044670633314259317030912.000000\n",
      "Total training time: 236.41 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 6958623892166.72, NNZs: 155839, Bias: 45702592.877846, T: 356994428, Avg. loss: 34878848216236208479535104.000000\n",
      "Total training time: 237.16 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 6944808131755.08, NNZs: 155839, Bias: -65339667.626381, T: 358180456, Avg. loss: 35015007986683375356215296.000000\n",
      "Total training time: 237.92 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 6949287674502.35, NNZs: 155839, Bias: -115472435.061817, T: 359366484, Avg. loss: 34963923949977336195055616.000000\n",
      "Total training time: 238.69 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 6944187092102.26, NNZs: 155839, Bias: -115954852.578382, T: 360552512, Avg. loss: 34883573692205632979992576.000000\n",
      "Total training time: 239.45 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 6955854632055.40, NNZs: 155839, Bias: -54118847.736811, T: 361738540, Avg. loss: 34933314150797903520595968.000000\n",
      "Total training time: 240.22 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 6912583153181.91, NNZs: 155839, Bias: -9247482.564884, T: 362924568, Avg. loss: 34781035257801819423768576.000000\n",
      "Total training time: 240.97 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 6933144831425.96, NNZs: 155839, Bias: -131631035.704250, T: 364110596, Avg. loss: 34716557873919152411901952.000000\n",
      "Total training time: 241.72 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 6910698615336.35, NNZs: 155839, Bias: -3856447.769143, T: 365296624, Avg. loss: 34845284002277230666317824.000000\n",
      "Total training time: 242.47 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 6907509328062.62, NNZs: 155839, Bias: -73033984.226152, T: 366482652, Avg. loss: 34608478689090331132035072.000000\n",
      "Total training time: 243.24 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 6892824431674.55, NNZs: 155839, Bias: -46719710.280173, T: 367668680, Avg. loss: 34498123755739417950552064.000000\n",
      "Total training time: 244.01 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 6912845405463.23, NNZs: 155839, Bias: -25980066.515783, T: 368854708, Avg. loss: 34472136808786301798055936.000000\n",
      "Total training time: 244.76 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 6902256264789.34, NNZs: 155839, Bias: -28470809.818407, T: 370040736, Avg. loss: 34462938316743558157041664.000000\n",
      "Total training time: 245.50 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 6885953235026.94, NNZs: 155839, Bias: -41854596.116871, T: 371226764, Avg. loss: 34215207954645311320752128.000000\n",
      "Total training time: 246.24 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 6854036812946.42, NNZs: 155839, Bias: -53122776.461754, T: 372412792, Avg. loss: 34483781254627861624520704.000000\n",
      "Total training time: 246.98 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 6881877264581.68, NNZs: 155839, Bias: 1818063.818587, T: 373598820, Avg. loss: 34264835694850074350714880.000000\n",
      "Total training time: 247.73 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 6838811660870.56, NNZs: 155839, Bias: -52704448.552051, T: 374784848, Avg. loss: 34316421829245912416780288.000000\n",
      "Total training time: 248.50 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 6868735174028.65, NNZs: 155839, Bias: -48801010.025006, T: 375970876, Avg. loss: 34099054362285165265289216.000000\n",
      "Total training time: 249.27 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 6861848639762.39, NNZs: 155839, Bias: -3634442.965254, T: 377156904, Avg. loss: 34121567230489809270079488.000000\n",
      "Total training time: 250.03 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 6852304259641.53, NNZs: 155839, Bias: 9145555.785225, T: 378342932, Avg. loss: 33905734998616490693885952.000000\n",
      "Total training time: 250.77 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 6835755482612.33, NNZs: 155839, Bias: -71125465.557012, T: 379528960, Avg. loss: 33883882392474887397572608.000000\n",
      "Total training time: 251.53 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 6850409158946.45, NNZs: 155839, Bias: 61824994.830551, T: 380714988, Avg. loss: 33888737039491546170261504.000000\n",
      "Total training time: 252.28 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 6831379598834.50, NNZs: 155839, Bias: -36669782.244768, T: 381901016, Avg. loss: 33876541576655673843777536.000000\n",
      "Total training time: 253.05 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 6824186898806.04, NNZs: 155839, Bias: 6739239.959221, T: 383087044, Avg. loss: 33715328541471852732612608.000000\n",
      "Total training time: 253.82 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 6815053040587.79, NNZs: 155839, Bias: 34517440.319474, T: 384273072, Avg. loss: 33812488966717501127786496.000000\n",
      "Total training time: 254.58 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 6832755394638.63, NNZs: 155839, Bias: 51485023.033923, T: 385459100, Avg. loss: 33633719232671583897124864.000000\n",
      "Total training time: 255.32 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 6794158825949.78, NNZs: 155839, Bias: -85979063.513377, T: 386645128, Avg. loss: 33663723738798416887545856.000000\n",
      "Total training time: 256.08 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 6816596606485.94, NNZs: 155839, Bias: -52238106.828742, T: 387831156, Avg. loss: 33454859277846351087403008.000000\n",
      "Total training time: 256.84 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 6789410385340.16, NNZs: 155839, Bias: -66117217.499634, T: 389017184, Avg. loss: 33581638141149112965267456.000000\n",
      "Total training time: 257.60 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 6810031366337.67, NNZs: 155839, Bias: -2368223.749991, T: 390203212, Avg. loss: 33299641272967424301334528.000000\n",
      "Total training time: 258.37 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 6792578562939.87, NNZs: 155839, Bias: -48491008.822193, T: 391389240, Avg. loss: 33479753940045802799890432.000000\n",
      "Total training time: 259.15 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 6781845821205.84, NNZs: 155839, Bias: -19605125.797191, T: 392575268, Avg. loss: 33264196605973484810534912.000000\n",
      "Total training time: 259.90 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 6785054881261.53, NNZs: 155839, Bias: 28723095.951172, T: 393761296, Avg. loss: 33339369620187780688642048.000000\n",
      "Total training time: 260.64 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 6776944590833.25, NNZs: 155839, Bias: -25723275.148176, T: 394947324, Avg. loss: 33198387188232727119790080.000000\n",
      "Total training time: 261.40 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 6764012683515.26, NNZs: 155839, Bias: -53815453.695222, T: 396133352, Avg. loss: 33122338456768840537735168.000000\n",
      "Total training time: 262.16 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 6759034923476.09, NNZs: 155839, Bias: -5848796.789449, T: 397319380, Avg. loss: 33130661785024016246898688.000000\n",
      "Total training time: 262.92 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 6743754377711.08, NNZs: 155839, Bias: -23169370.854149, T: 398505408, Avg. loss: 33018701517770725271797760.000000\n",
      "Total training time: 263.67 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 6752415083685.66, NNZs: 155839, Bias: -110447936.297769, T: 399691436, Avg. loss: 32994865755859976611430400.000000\n",
      "Total training time: 264.43 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 6742583337032.02, NNZs: 155839, Bias: -35548594.299924, T: 400877464, Avg. loss: 32926454462551453931143168.000000\n",
      "Total training time: 265.19 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 6748913698293.34, NNZs: 155839, Bias: 6261011.427924, T: 402063492, Avg. loss: 32838990330505021106421760.000000\n",
      "Total training time: 265.93 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 6733536847281.85, NNZs: 155839, Bias: 4490480.186981, T: 403249520, Avg. loss: 32798793930970262293970944.000000\n",
      "Total training time: 266.69 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 6730650680619.40, NNZs: 155839, Bias: -46678692.575840, T: 404435548, Avg. loss: 32914742217245136074571776.000000\n",
      "Total training time: 267.44 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 6719221761078.93, NNZs: 155839, Bias: 13478789.820973, T: 405621576, Avg. loss: 32681963872752446981799936.000000\n",
      "Total training time: 268.20 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 6713351281988.93, NNZs: 155839, Bias: 27511160.683413, T: 406807604, Avg. loss: 32730675493183014408552448.000000\n",
      "Total training time: 268.95 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 6699685979757.10, NNZs: 155839, Bias: -62639324.234414, T: 407993632, Avg. loss: 32739008936540390759071744.000000\n",
      "Total training time: 269.72 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 6709492728381.68, NNZs: 155839, Bias: -45451971.230368, T: 409179660, Avg. loss: 32694978762073733437849600.000000\n",
      "Total training time: 270.47 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 6690299511912.01, NNZs: 155839, Bias: -125581813.121482, T: 410365688, Avg. loss: 32764228863878445035159552.000000\n",
      "Total training time: 271.22 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 6685295327498.53, NNZs: 155839, Bias: -19760600.736044, T: 411551716, Avg. loss: 32561102952931804149972992.000000\n",
      "Total training time: 271.97 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 6689351553647.43, NNZs: 155839, Bias: -3651961.716367, T: 412737744, Avg. loss: 32460175702753142158393344.000000\n",
      "Total training time: 272.71 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 6660417813545.13, NNZs: 155839, Bias: 343317.069465, T: 413923772, Avg. loss: 32498209517855378694471680.000000\n",
      "Total training time: 273.47 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 6679534422333.69, NNZs: 155839, Bias: 25242614.784999, T: 415109800, Avg. loss: 32349237719405275778121728.000000\n",
      "Total training time: 274.23 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 6665005209121.37, NNZs: 155839, Bias: 59231534.570344, T: 416295828, Avg. loss: 32254882760625429269184512.000000\n",
      "Total training time: 274.98 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 6671853561299.62, NNZs: 155839, Bias: -78650202.344444, T: 417481856, Avg. loss: 32393863303087393110753280.000000\n",
      "Total training time: 275.74 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 6662766720089.48, NNZs: 155839, Bias: 34654070.954290, T: 418667884, Avg. loss: 32223122295499351356604416.000000\n",
      "Total training time: 276.49 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 6651484128770.60, NNZs: 155839, Bias: 7695129.986034, T: 419853912, Avg. loss: 32393382399246292815446016.000000\n",
      "Total training time: 277.24 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 6666311879374.22, NNZs: 155839, Bias: -104894389.598766, T: 421039940, Avg. loss: 32209366593061401365839872.000000\n",
      "Total training time: 278.00 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 6632638407501.61, NNZs: 155839, Bias: -12496786.540619, T: 422225968, Avg. loss: 32143872578730394887651328.000000\n",
      "Total training time: 278.76 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 6627058380781.61, NNZs: 155839, Bias: -54438043.731956, T: 423411996, Avg. loss: 32106022216364300328501248.000000\n",
      "Total training time: 279.51 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 6629330855534.95, NNZs: 155839, Bias: 33537834.222505, T: 424598024, Avg. loss: 31981455984473462299164672.000000\n",
      "Total training time: 280.27 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 6624752848556.60, NNZs: 155839, Bias: 76823701.451587, T: 425784052, Avg. loss: 31910439560501702202228736.000000\n",
      "Total training time: 281.02 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 6636885817544.78, NNZs: 155839, Bias: -62458171.930564, T: 426970080, Avg. loss: 31894386227137106319245312.000000\n",
      "Total training time: 281.76 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 6622180563781.51, NNZs: 155839, Bias: -79486793.450370, T: 428156108, Avg. loss: 31957678548602662179307520.000000\n",
      "Total training time: 282.52 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 6614360218881.34, NNZs: 155839, Bias: 2347922.470093, T: 429342136, Avg. loss: 31793535724666583487873024.000000\n",
      "Total training time: 283.27 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 6625182731475.49, NNZs: 155839, Bias: -17823120.326991, T: 430528164, Avg. loss: 31800660229991045879824384.000000\n",
      "Total training time: 284.03 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 6611570332613.32, NNZs: 155839, Bias: -26479840.201293, T: 431714192, Avg. loss: 31755052031520146221694976.000000\n",
      "Total training time: 284.79 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 6606461875498.23, NNZs: 155839, Bias: 2352030.246060, T: 432900220, Avg. loss: 31977550105351301114101760.000000\n",
      "Total training time: 285.55 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 6611789045960.15, NNZs: 155839, Bias: 45387895.536365, T: 434086248, Avg. loss: 31572848279354906098270208.000000\n",
      "Total training time: 286.31 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 6596526416352.31, NNZs: 155839, Bias: -103338219.009888, T: 435272276, Avg. loss: 31539409234485613596508160.000000\n",
      "Total training time: 287.06 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 6595874790394.97, NNZs: 155839, Bias: -1956445.676313, T: 436458304, Avg. loss: 31535734359376772743561216.000000\n",
      "Total training time: 287.94 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 6585389799804.87, NNZs: 155839, Bias: -31713581.636586, T: 437644332, Avg. loss: 31405282674589136756670464.000000\n",
      "Total training time: 288.70 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 6565281147906.98, NNZs: 155839, Bias: -93707839.517270, T: 438830360, Avg. loss: 31467704256859492910628864.000000\n",
      "Total training time: 289.45 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 6571372945602.66, NNZs: 155839, Bias: -35426968.377544, T: 440016388, Avg. loss: 31344250770805350480216064.000000\n",
      "Total training time: 290.19 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 6568429056719.69, NNZs: 155839, Bias: 33128592.841626, T: 441202416, Avg. loss: 31483237763692389092818944.000000\n",
      "Total training time: 290.93 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 6542232113950.32, NNZs: 155839, Bias: -1099013.521817, T: 442388444, Avg. loss: 31317160570508585673424896.000000\n",
      "Total training time: 291.68 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 6563242618591.75, NNZs: 155839, Bias: -43881012.756490, T: 443574472, Avg. loss: 31088715501996591280029696.000000\n",
      "Total training time: 292.43 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 6564512616218.55, NNZs: 155839, Bias: -59386233.636479, T: 444760500, Avg. loss: 31274029784829098935189504.000000\n",
      "Total training time: 293.18 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 6549973963183.52, NNZs: 155839, Bias: -33437039.944512, T: 445946528, Avg. loss: 31183875563663378524143616.000000\n",
      "Total training time: 293.94 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 6536808213189.75, NNZs: 155839, Bias: -75573028.398861, T: 447132556, Avg. loss: 31050421423754357222932480.000000\n",
      "Total training time: 294.69 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 6529388931045.17, NNZs: 155839, Bias: -75216815.584427, T: 448318584, Avg. loss: 31170564616714130198364160.000000\n",
      "Total training time: 295.42 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 6551983320143.05, NNZs: 155839, Bias: -97600506.036245, T: 449504612, Avg. loss: 31206661729765491715604480.000000\n",
      "Total training time: 296.17 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 6514100124763.92, NNZs: 155839, Bias: 2939238.689202, T: 450690640, Avg. loss: 30951514455712810657644544.000000\n",
      "Total training time: 296.92 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 6545139468507.96, NNZs: 155839, Bias: 28417270.107025, T: 451876668, Avg. loss: 30872314804685790328651776.000000\n",
      "Total training time: 297.66 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 6510396738803.26, NNZs: 155839, Bias: -34734885.822915, T: 453062696, Avg. loss: 30969813631145550947549184.000000\n",
      "Total training time: 298.41 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 6514117681654.84, NNZs: 155839, Bias: -58291054.733335, T: 454248724, Avg. loss: 30838145858386355147505664.000000\n",
      "Total training time: 299.16 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 6523474594148.85, NNZs: 155839, Bias: -84859873.304204, T: 455434752, Avg. loss: 30815036684592832560758784.000000\n",
      "Total training time: 299.91 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 6521128612949.22, NNZs: 155839, Bias: -38304101.130017, T: 456620780, Avg. loss: 30931367075747669041217536.000000\n",
      "Total training time: 300.65 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 6510359172457.08, NNZs: 155839, Bias: 32422450.842925, T: 457806808, Avg. loss: 30912805229739852223741952.000000\n",
      "Total training time: 301.41 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 6506207934470.37, NNZs: 155839, Bias: 9548014.896519, T: 458992836, Avg. loss: 30601924396830154572169216.000000\n",
      "Total training time: 302.16 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 6503978831493.76, NNZs: 155839, Bias: -27822693.507116, T: 460178864, Avg. loss: 30570657592807994196754432.000000\n",
      "Total training time: 302.91 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 6519524613353.17, NNZs: 155839, Bias: 23803360.529028, T: 461364892, Avg. loss: 30506568297800708766302208.000000\n",
      "Total training time: 303.66 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 6486802691988.65, NNZs: 155839, Bias: -29572763.320905, T: 462550920, Avg. loss: 30675658953048578162950144.000000\n",
      "Total training time: 304.42 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 6505086220423.32, NNZs: 155839, Bias: -140449280.068155, T: 463736948, Avg. loss: 30515542268324074083057664.000000\n",
      "Total training time: 305.15 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 6506216250390.68, NNZs: 155839, Bias: -60965452.149022, T: 464922976, Avg. loss: 30691605245657765363318784.000000\n",
      "Total training time: 305.89 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 6482001490245.19, NNZs: 155839, Bias: -73627074.400025, T: 466109004, Avg. loss: 30483128653655163334033408.000000\n",
      "Total training time: 306.63 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 6460558884484.06, NNZs: 155839, Bias: -37646887.163951, T: 467295032, Avg. loss: 30479012876005518574878720.000000\n",
      "Total training time: 307.38 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 6480386452586.80, NNZs: 155839, Bias: -3620608.246477, T: 468481060, Avg. loss: 30317647699644625167843328.000000\n",
      "Total training time: 308.13 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 6462683871588.06, NNZs: 155839, Bias: -34506105.580907, T: 469667088, Avg. loss: 30508513970033937036083200.000000\n",
      "Total training time: 308.89 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 6464633699163.91, NNZs: 155839, Bias: 57324605.228994, T: 470853116, Avg. loss: 30160217302462648375312384.000000\n",
      "Total training time: 309.68 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 6468010726871.52, NNZs: 155839, Bias: 73414660.889050, T: 472039144, Avg. loss: 30209494830141880534040576.000000\n",
      "Total training time: 310.43 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 6460138699173.96, NNZs: 155839, Bias: -7942905.703765, T: 473225172, Avg. loss: 30287019134009302367338496.000000\n",
      "Total training time: 311.19 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 6431309765002.65, NNZs: 155839, Bias: -57582831.339255, T: 474411200, Avg. loss: 30205551824785537518010368.000000\n",
      "Total training time: 311.95 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 6439369458148.77, NNZs: 155839, Bias: 22094905.261181, T: 475597228, Avg. loss: 30126699694894018096791552.000000\n",
      "Total training time: 312.71 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 6436539119787.23, NNZs: 155839, Bias: 5755208.003570, T: 476783256, Avg. loss: 30225734105054309613830144.000000\n",
      "Total training time: 313.46 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 6452470201763.44, NNZs: 155839, Bias: -98522922.528358, T: 477969284, Avg. loss: 30007318787418022979043328.000000\n",
      "Total training time: 314.20 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 6420066596705.90, NNZs: 155839, Bias: -110665865.207843, T: 479155312, Avg. loss: 30115912051260919134552064.000000\n",
      "Total training time: 314.95 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 6425688868451.62, NNZs: 155839, Bias: -65162357.524299, T: 480341340, Avg. loss: 30025465606388249102647296.000000\n",
      "Total training time: 315.71 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 6415871506728.76, NNZs: 155839, Bias: -37900519.305623, T: 481527368, Avg. loss: 30010571080464928425902080.000000\n",
      "Total training time: 316.47 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 6437595497485.95, NNZs: 155839, Bias: -34220303.794773, T: 482713396, Avg. loss: 29710033671224110773960704.000000\n",
      "Total training time: 317.23 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 6436463966231.25, NNZs: 155839, Bias: -40372255.220720, T: 483899424, Avg. loss: 29905238034728753911824384.000000\n",
      "Total training time: 317.99 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 6404928958198.25, NNZs: 155839, Bias: 10401157.580283, T: 485085452, Avg. loss: 29875512358421931690557440.000000\n",
      "Total training time: 318.77 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 6382673054270.30, NNZs: 155839, Bias: 395098.724861, T: 486271480, Avg. loss: 29842091990743157395423232.000000\n",
      "Total training time: 319.55 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 6395230907046.21, NNZs: 155839, Bias: -6887334.292561, T: 487457508, Avg. loss: 29614721329463223421763584.000000\n",
      "Total training time: 320.31 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 6417870192632.22, NNZs: 155839, Bias: -48402387.266174, T: 488643536, Avg. loss: 29595136160464652847808512.000000\n",
      "Total training time: 321.06 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 6386114950997.73, NNZs: 155839, Bias: -48195331.281212, T: 489829564, Avg. loss: 29776138519434490227458048.000000\n",
      "Total training time: 321.80 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 6360902335269.76, NNZs: 155839, Bias: 3036793.113114, T: 491015592, Avg. loss: 29647744825291974532661248.000000\n",
      "Total training time: 322.55 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 6390790486302.42, NNZs: 155839, Bias: -59446904.126854, T: 492201620, Avg. loss: 29556144399190753554726912.000000\n",
      "Total training time: 323.32 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 6361392639249.40, NNZs: 155839, Bias: -8441698.109335, T: 493387648, Avg. loss: 29614470720211123670351872.000000\n",
      "Total training time: 324.09 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 6393496172781.01, NNZs: 155839, Bias: -21837519.786124, T: 494573676, Avg. loss: 29524441254629205113569280.000000\n",
      "Total training time: 324.84 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 6386254561035.44, NNZs: 155839, Bias: -16411813.517988, T: 495759704, Avg. loss: 29389656002858105475956736.000000\n",
      "Total training time: 325.59 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 6355288179944.94, NNZs: 155839, Bias: -90064353.342972, T: 496945732, Avg. loss: 29406047440602969397002240.000000\n",
      "Total training time: 326.35 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 6377372993762.23, NNZs: 155839, Bias: 29324716.800415, T: 498131760, Avg. loss: 29460036057386158382907392.000000\n",
      "Total training time: 327.12 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 6377288651209.20, NNZs: 155839, Bias: -67477768.339753, T: 499317788, Avg. loss: 29595545014614184456880128.000000\n",
      "Total training time: 327.88 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 6369876346971.52, NNZs: 155839, Bias: -59150655.630997, T: 500503816, Avg. loss: 29281764713159141055528960.000000\n",
      "Total training time: 328.65 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 6353548585589.73, NNZs: 155839, Bias: -45814873.549828, T: 501689844, Avg. loss: 29281135488759341716078592.000000\n",
      "Total training time: 329.41 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 6344054913066.54, NNZs: 155839, Bias: -82525239.603724, T: 502875872, Avg. loss: 29248863738185369876692992.000000\n",
      "Total training time: 330.17 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 6326223738744.05, NNZs: 155839, Bias: -32142887.514287, T: 504061900, Avg. loss: 29287300034123111514243072.000000\n",
      "Total training time: 330.93 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 6335807007687.42, NNZs: 155839, Bias: 59570581.509863, T: 505247928, Avg. loss: 29316133461765622029352960.000000\n",
      "Total training time: 331.67 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 6338645383954.25, NNZs: 155839, Bias: 74224047.696388, T: 506433956, Avg. loss: 29100873932703335620542464.000000\n",
      "Total training time: 332.42 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 6322536714076.50, NNZs: 155839, Bias: 9500427.350893, T: 507619984, Avg. loss: 29058709709678984418033664.000000\n",
      "Total training time: 333.18 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 6316382738967.31, NNZs: 155839, Bias: -71845505.205980, T: 508806012, Avg. loss: 29056618943558792448573440.000000\n",
      "Total training time: 333.94 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 6332884321043.86, NNZs: 155839, Bias: 43813533.744350, T: 509992040, Avg. loss: 28914507166774439259930624.000000\n",
      "Total training time: 334.70 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 6324497193609.33, NNZs: 155839, Bias: -16066034.723625, T: 511178068, Avg. loss: 29040004466070477915291648.000000\n",
      "Total training time: 335.44 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 6299731711600.75, NNZs: 155839, Bias: -42752865.647759, T: 512364096, Avg. loss: 28967062536904953565282304.000000\n",
      "Total training time: 336.20 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 6329674716817.47, NNZs: 155839, Bias: 38078844.680796, T: 513550124, Avg. loss: 28934162666797917121019904.000000\n",
      "Total training time: 336.95 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 6319283885019.14, NNZs: 155839, Bias: 17726216.159422, T: 514736152, Avg. loss: 29062248286327133056270336.000000\n",
      "Total training time: 337.68 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 6316943327431.26, NNZs: 155839, Bias: -35802760.278047, T: 515922180, Avg. loss: 29028803661561807060860928.000000\n",
      "Total training time: 338.45 seconds.\n",
      "Convergence after 435 epochs took 338.45 seconds\n",
      "CPU times: user 5min 32s, sys: 5.06 s, total: 5min 37s\n",
      "Wall time: 5min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n                ('sgdregressor', SGDRegressor(verbose=True))])",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n                (&#x27;sgdregressor&#x27;, SGDRegressor(verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n                (&#x27;sgdregressor&#x27;, SGDRegressor(verbose=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(verbose=True)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "def rmsle(y_pred, y_true):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_pred.clip(0)), np.expm1(y_true)))\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# model = make_pipeline(StandardScaler(with_mean=False), Ridge(\n",
    "#     solver='auto',\n",
    "#     fit_intercept=True,\n",
    "#     alpha=0.5,\n",
    "#     max_iter=100,\n",
    "#     tol=0.05\n",
    "# ))\n",
    "model = make_pipeline(StandardScaler(with_mean=True), SGDRegressor(verbose=True))\n",
    "# for train_ids, validation_ids in cv.split(X_train_transformed):\n",
    "#     model.partial_fit(X_train_transformed[train_ids], Y_train.values[train_ids])\n",
    "#     y_pred_validation = model.predict(X_train_transformed[validation_ids])\n",
    "#     error = rmsle(y_pred_validation, Y_train.values[validation_ids])\n",
    "#     print(f'Validation RMSLE = {error:.5f}')\n",
    "\n",
    "model.fit(X_train_transformed, Y_train.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.18421290e+11, -9.49173835e+12, -4.29868434e+12, ...,\n       -2.51483821e+13,  4.83050251e+12,  2.19834670e+12])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = model.predict(X_test_transformed)\n",
    "# error = rmsle(y_pred_validation, Y_test.values)\n",
    "# print(f'Validation RMSLE = {error:.5f}')\n",
    "y_pred_validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n            \n                \n                \n    \n        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n            <b>\n    \n        y\n    \n</b>\n\ntop features\n        </p>\n    \n    <table class=\"eli5-weights\"\n           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n        <thead>\n        <tr style=\"border: none;\">\n            \n                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n                    Weight<sup>?</sup>\n                </th>\n            \n            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n            \n        </tr>\n        </thead>\n        <tbody>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 85.72%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +115496825500.815\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__correctly for teeth\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 86.61%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +105358676254.622\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__lighters pokémon\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 86.83%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +102861973791.616\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__rm try our\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 87.29%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +97713075500.035\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__free iphone\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +88270767775.729\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__brandnew temper\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.48%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +85004382142.168\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__egyptian comfort\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.77%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +81940086352.979\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__sexy and get\n    </td>\n    \n</tr>\n        \n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.77%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 77863 more positive &hellip;</i>\n                </td>\n            </tr>\n        \n\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.76%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 77957 more negative &hellip;</i>\n                </td>\n            </tr>\n        \n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.76%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -82040250747.467\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__listing for 30oz\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.70%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -82601514731.740\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__to melt and\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.70%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -82664461043.892\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__for phone stand\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.56%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -84106065577.356\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__zoeva makeup\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -88267509053.765\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__4x8 pink\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.01%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -89998145966.719\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__6oz african\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 87.51%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -95372801079.333\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__advisory iphone\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 87.44%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -96175358943.598\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__8oz african\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 87.37%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -96938577583.333\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__included search jaybird\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 86.95%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -101519485959.416\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__henna filled\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 86.73%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -103947183108.348\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__in the appropriate\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 84.34%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -131677558504.733\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_description__46 to 48\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -186835019285.246\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__bongs water\n    </td>\n    \n</tr>\n        \n\n        </tbody>\n    </table>\n\n            \n        \n\n        \n\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(model, vec=vectorizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eli5.show_weights(model, vec=vectorizer, top=100, feature_filter=lambda x: x != '<BIAS>')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE for ridge regression = 0.46513\n"
     ]
    }
   ],
   "source": [
    "ridge_model = make_pipeline(StandardScaler(with_mean=False), Ridge(\n",
    "    solver='auto',\n",
    "    fit_intercept=True,\n",
    "    alpha=0.5,\n",
    "    max_iter=100,\n",
    "    tol=0.05,\n",
    "))\n",
    "ridge_model.fit(X_train_transformed, Y_train.values)\n",
    "\n",
    "y_pred_validation_ridge = ridge_model.predict(X_test_transformed)\n",
    "ridge_error = rmsle(y_pred_validation_ridge, Y_test.values)\n",
    "print(f'Validation RMSLE for ridge regression = {ridge_error:.5f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "0.08284415584806659"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation_ridge.min()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n        \n\n    \n\n        \n            \n                \n                \n    \n        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n            <b>\n    \n        y\n    \n</b>\n\ntop features\n        </p>\n    \n    <table class=\"eli5-weights\"\n           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n        <thead>\n        <tr style=\"border: none;\">\n            \n                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n                    Weight<sup>?</sup>\n                </th>\n            \n            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n            \n        </tr>\n        </thead>\n        <tbody>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.062\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__bundle\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 83.42%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.047\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__men/shoes/athletic\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 83.45%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.047\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_condition_id__1\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 84.03%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.045\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__electronics/cell phones &amp; accessories/cell phones &amp; smartphones\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 84.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.044\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        shipping__0\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 84.72%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.042\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__lularoe\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 85.43%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.039\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__lululemon\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 85.86%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.038\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__kendra scott\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 86.06%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.037\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/shoes/athletic\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 86.79%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.034\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/shoes/boots\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 87.21%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.032\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__electronics/video games &amp; consoles/consoles\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 87.56%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.031\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__louis vuitton\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 87.59%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.031\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/women&#x27;s handbags/shoulder bag\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.06%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.029\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__lululemon\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 88.92%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.026\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/shoes/fashion sneakers\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.22%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.025\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__men/shoes/fashion sneakers\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.72%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.024\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__tiffany &amp; co.\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.74%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.024\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__gucci\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.81%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__electronics/computers &amp; tablets/laptops &amp; netbooks\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 89.94%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__michael kors\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.05%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/athletic apparel/pants, tights, leggings\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.30%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.022\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__jordan\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.40%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.022\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__carly\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.44%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__air jordan\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.48%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__kids/boys (4+)/shoes\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.50%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__coach\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.56%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__burberry\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.78%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/women&#x27;s handbags/messenger &amp; crossbody\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.90%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__gucci\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 90.91%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__14k\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 91.16%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.019\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__lot\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 91.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.019\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__tory burch\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(120, 100.00%, 91.27%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        +0.019\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__sarah\n    </td>\n    \n</tr>\n        \n        \n            <tr style=\"background-color: hsl(120, 100.00%, 91.27%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 82247 more positive &hellip;</i>\n                </td>\n            </tr>\n        \n\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 91.23%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 73542 more negative &hellip;</i>\n                </td>\n            </tr>\n        \n        \n            <tr style=\"background-color: hsl(0, 100.00%, 91.23%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.019\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/jewelry/earrings\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 91.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.019\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__choker\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.96%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__shirt\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.94%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__beauty/makeup/eyes\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.88%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__beauty/makeup/lips\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.80%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.020\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__stickers\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.69%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__women/underwear/panties\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.60%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__electronics/media/dvd\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.55%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.021\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__handmade/paper goods/sticker\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 90.01%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__sticker\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 89.97%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__forever 21\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 89.85%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.023\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        name__case\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 88.70%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.027\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        category_name__electronics/cell phones &amp; accessories/cases, covers &amp; skins\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 86.16%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.036\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_condition_id__3\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 84.64%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.042\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        brand_name__missing\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 84.40%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.043\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        item_condition_id__4\n    </td>\n    \n</tr>\n        \n            <tr style=\"background-color: hsl(0, 100.00%, 84.17%); border: none;\">\n    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n        -0.044\n    </td>\n    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n        shipping__1\n    </td>\n    \n</tr>\n        \n\n        </tbody>\n    </table>\n\n            \n        \n\n        \n\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(ridge_model, vec=vectorizer, top=50, feature_filter=lambda x: x != '<BIAS>')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
